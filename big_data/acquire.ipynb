{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrameReader.parquet of <pyspark.sql.readwriter.DataFrameReader object at 0x106f14c88>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, column, expr\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nprocs = multiprocessing.cpu_count()\n",
    "\n",
    "spark = (pyspark.sql.SparkSession.builder\n",
    " .master('local')\n",
    " .config('spark.jars.packages', 'mysql:mysql-connector-java:8.0.16')\n",
    " .config('spark.driver.memory', '4G')\n",
    " .config('spark.driver.cores', nprocs)\n",
    " .config('spark.sql.shuffle.partitions', nprocs)\n",
    " .appName('MySparkApplication')\n",
    " .getOrCreate())\n",
    "\n",
    "spark.read.json\n",
    "spark.read.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "Using case.csv & dept.csv:\n",
    "\n",
    "read into spark environment (df_case, df_dept)\n",
    "\n",
    "write df_case and df_dept back to disk into their own directories (my_cases and my_depts)\n",
    "\n",
    "Write df_case and df_dept to parquet files (my_cases_parquet and my_depts_parquet)\n",
    "\n",
    "Read your parquet files back into your spark environment.\n",
    "\n",
    "Read case.csv and dept.csv into a pandas dataframe. (cases_pdf, depts_pdf)\n",
    "\n",
    "Convert the pandas dataframes into spark dataframes (cases_sdf, depts_sdf)\n",
    "\n",
    "Convert the spark dataframes back into pandas dataframes. (cases_pdf1, depts_pdf1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use this if want to explicitly load columns, not when \"inferschema\"\n",
    "case_schema = T.StructType([\n",
    "    T.StructField(\"case_id\", T.StringType()),\n",
    "    T.StructField(\"case_opened\", T.DateType()),\n",
    "    T.StructField(\"case_closed_date\", T.DateType()),\n",
    "    T.StructField(\"SLA_due_date\", T.DateType()),\n",
    "    T.StructField(\"case_late\", T.BooleanType()),\n",
    "    T.StructField(\"num_days_late\", T.FloatType()),\n",
    "    T.StructField(\"case_closed\", T.BooleanType()),\n",
    "    T.StructField(\"dept_division\", T.StringType()),\n",
    "    T.StructField(\"service_request_type\", T.StringType()),\n",
    "    T.StructField(\"SLA_days\", T.FloatType()),\n",
    "    T.StructField(\"case_status\", T.StringType()),\n",
    "    T.StructField(\"source_id\", T.StringType()),\n",
    "    T.StructField(\"request_address\", T.StringType()),\n",
    "    T.StructField(\"council_district\", T.StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case = (spark.read\n",
    " .option('header', True)\n",
    " .option('inferSchema', True)\n",
    "# .option('schema', case_schema)          \n",
    " .format('csv')\n",
    " .load('./sa311/case.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept = (spark.read\n",
    " .option('header', True)\n",
    " .option('inferSchema', True)\n",
    " .format('csv')\n",
    " .load('./sa311/dept.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source = (spark.read\n",
    " .option('header', True)\n",
    " .option('inferSchema', True)\n",
    " .format('csv')\n",
    " .load('./sa311/source.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date|SLA_due_date|case_late|      num_days_late|case_closed|   dept_division|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|1014127332|     1/1/18 0:42|    1/1/18 12:29|9/26/20 0:42|       NO| -998.5087616000001|        YES|Field Operations|        Stray Animal|      999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|               5|\n",
      "|1014127333|     1/1/18 0:46|     1/3/18 8:11| 1/5/18 8:30|       NO|-2.0126041669999997|        YES|     Storm Water|Removal Of Obstru...|4.322222222|     Closed| svcCRMSS|2215  GOLIAD RD, ...|               3|\n",
      "|1014127334|     1/1/18 0:48|     1/2/18 7:57| 1/5/18 8:30|       NO|       -3.022337963|        YES|     Storm Water|Removal Of Obstru...|4.320729167|     Closed| svcCRMSS|102  PALFREY ST W...|               3|\n",
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_case.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write df_case and df_dept back to disk into their own directories (my_cases and my_depts)\n",
    "\n",
    "### Write df_case and df_dept to parquet files (my_cases_parquet and my_depts_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case.write.format('com.databricks.spark.csv') \\\n",
    "  .mode('overwrite').option(\"header\", \"true\").save('./sa311/my_cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept.write.format('com.databricks.spark.csv') \\\n",
    "  .mode('overwrite').option(\"header\", \"true\").save('./sa311/my_depts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case.write.format('parquet').mode('overwrite').\\\n",
    "    option('header','true').save('./sa311/my_cases_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept.write.format('parquet').mode('overwrite').\\\n",
    "    option('header','true').save('./sa311/my_dept_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read your parquet files back into your spark environment.\n",
    "\n",
    "### Read case.csv and dept.csv into a pandas dataframe. (cases_pdf, depts_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_parquet_df = spark.read.format('parquet').\\\n",
    "    option(\"header\", True).\\\n",
    "    option(\"inferSchema\", True).\\\n",
    "    load(\"./sa311/my_cases_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dept_parquet_df = spark.read.format('parquet').\\\n",
    "    option(\"header\", True).\\\n",
    "    option(\"inferSchema\", True).\\\n",
    "    load(\"./sa311/my_dept_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(case_id=1014127332, case_opened_date='1/1/18 0:42', case_closed_date='1/1/18 12:29', SLA_due_date='9/26/20 0:42', case_late='NO', num_days_late=-998.5087616000001, case_closed='YES', dept_division='Field Operations', service_request_type='Stray Animal', SLA_days=999.0, case_status='Closed', source_id='svcCRMLS', request_address='2315  EL PASO ST, San Antonio, 78207', council_district=5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_parquet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read case.csv and dept.csv into a pandas dataframe. (cases_pdf, depts_pdf)\n",
    "\n",
    "### Convert the pandas dataframes into spark dataframes (cases_sdf, depts_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_pdf = pd.read_csv('./sa311/case.csv', sep=\",\")\n",
    "depts_pdf = pd.read_csv('./sa311/dept.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to a spark dataframe from pandas\n",
    "# spark conversion doesn't know what to do with nulls, so\n",
    "# 1st - convert empty strings, or blanks -  to nans\n",
    "# 2nd - drop the null values\n",
    "cases_pdf.replace('', np.nan, inplace=True)\n",
    "cases_pdf.replace(' ', np.nan, inplace=True)\n",
    "cases_pdf = cases_pdf.dropna(how='any',axis=0)\n",
    "depts_pdf.replace('', np.nan, inplace=True)\n",
    "depts_pdf.replace(' ', np.nan, inplace=True)\n",
    "depts_pdf = depts_pdf.dropna(how='any',axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_sdf = spark.createDataFrame(cases_pdf)\n",
    "depts_sdf = spark.createDataFrame(depts_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(case_id=1014127332, case_opened_date='1/1/18 0:42', case_closed_date='1/1/18 12:29', SLA_due_date='9/26/20 0:42', case_late='NO', num_days_late=-998.5087616, case_closed='YES', dept_division='Field Operations', service_request_type='Stray Animal', SLA_days=999.0, case_status='Closed', source_id='svcCRMLS', request_address='2315  EL PASO ST, San Antonio, 78207', council_district=5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_sdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the spark dataframes back into pandas dataframes. (cases_pdf1, depts_pdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_pdf1 = cases_sdf.toPandas()\n",
    "depts_pdf1 = depts_sdf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Inspect\n",
    "\n",
    "Read the 311 case data into a Spark DataFrame.\n",
    "Inspect the DataFrame. Are the data types for each column appropriate? Cast the data to appropriate types as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this will override some built-in python functions\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case = (spark.read\n",
    " .option('header', True)\n",
    " .option('inferSchema', True)\n",
    "# .option('schema', case_schema)          \n",
    " .format('csv')\n",
    " .load('./sa311/case.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "841704"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  how many rows\n",
    "df_case.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- case_opened_date: string (nullable = true)\n",
      " |-- case_closed_date: string (nullable = true)\n",
      " |-- SLA_due_date: string (nullable = true)\n",
      " |-- case_late: string (nullable = true)\n",
      " |-- num_days_late: double (nullable = true)\n",
      " |-- case_closed: string (nullable = true)\n",
      " |-- dept_division: string (nullable = true)\n",
      " |-- service_request_type: string (nullable = true)\n",
      " |-- SLA_days: double (nullable = true)\n",
      " |-- case_status: string (nullable = true)\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- request_address: string (nullable = true)\n",
      " |-- council_district: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# what are the data types\n",
    "df_case.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the nulls, now what is the count\n",
    "df_case = df_case.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "823590"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_case.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_case = df_case.withColumn(\"case_opened_date\", (col(\"case_opened_date\").cast(\"timestamp\")))\n",
    "df3_case = df2_case.withColumn(\"case_closed_date\", (col(\"case_closed_date\").cast(\"timestamp\")))\n",
    "spark_case_df = df3_case.withColumn(\"SLA_due_date\", (col(\"SLA_due_date\").cast(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- case_opened_date: timestamp (nullable = true)\n",
      " |-- case_closed_date: timestamp (nullable = true)\n",
      " |-- SLA_due_date: timestamp (nullable = true)\n",
      " |-- case_late: string (nullable = true)\n",
      " |-- num_days_late: double (nullable = true)\n",
      " |-- case_closed: string (nullable = true)\n",
      " |-- dept_division: string (nullable = true)\n",
      " |-- service_request_type: string (nullable = true)\n",
      " |-- SLA_days: double (nullable = true)\n",
      " |-- case_status: string (nullable = true)\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- request_address: string (nullable = true)\n",
      " |-- council_district: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_case_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
