{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrameReader.parquet of <pyspark.sql.readwriter.DataFrameReader object at 0x11d206588>>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, column, expr\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nprocs = multiprocessing.cpu_count()\n",
    "\n",
    "spark = (pyspark.sql.SparkSession.builder\n",
    " .master('local')\n",
    " .config('spark.jars.packages', 'mysql:mysql-connector-java:8.0.16')\n",
    " .config('spark.driver.memory', '4G')\n",
    " .config('spark.driver.cores', nprocs)\n",
    " .config('spark.sql.shuffle.partitions', nprocs)\n",
    " .appName('MySparkApplication')\n",
    " .getOrCreate())\n",
    "\n",
    "spark.read.json\n",
    "spark.read.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "Using case.csv & dept.csv:\n",
    "\n",
    "read into spark environment (df_case, df_dept)\n",
    "\n",
    "write df_case and df_dept back to disk into their own directories (my_cases and my_depts)\n",
    "\n",
    "Write df_case and df_dept to parquet files (my_cases_parquet and my_depts_parquet)\n",
    "\n",
    "Read your parquet files back into your spark environment.\n",
    "\n",
    "Read case.csv and dept.csv into a pandas dataframe. (cases_pdf, depts_pdf)\n",
    "\n",
    "Convert the pandas dataframes into spark dataframes (cases_sdf, depts_sdf)\n",
    "\n",
    "Convert the spark dataframes back into pandas dataframes. (cases_pdf1, depts_pdf1)\n",
    "\n",
    "Write the spark dataframes (cases_sdf, depts_sdf) to Hive tables.\n",
    "\n",
    "Explore the Hive database/tables you have created using the methods in the lesson.\n",
    "\n",
    "Read from the tables into two spark dataframes (cases_sdf, depts_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use this if want to explicitly load columns, not when \"inferschema\"\n",
    "case_schema = T.StructType([\n",
    "    T.StructField(\"case_id\", T.StringType()),\n",
    "    T.StructField(\"case_opened\", T.DateType()),\n",
    "    T.StructField(\"case_closed_date\", T.DateType()),\n",
    "    T.StructField(\"SLA_due_date\", T.DateType()),\n",
    "    T.StructField(\"case_late\", T.BooleanType()),\n",
    "    T.StructField(\"num_days_late\", T.FloatType()),\n",
    "    T.StructField(\"case_closed\", T.BooleanType()),\n",
    "    T.StructField(\"dept_division\", T.StringType()),\n",
    "    T.StructField(\"service_request_type\", T.StringType()),\n",
    "    T.StructField(\"SLA_days\", T.FloatType()),\n",
    "    T.StructField(\"case_status\", T.StringType()),\n",
    "    T.StructField(\"source_id\", T.StringType()),\n",
    "    T.StructField(\"request_address\", T.StringType()),\n",
    "    T.StructField(\"council_district\", T.StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case = (spark.read\n",
    " .option('header', True)\n",
    " .option('inferSchema', True)\n",
    "# .option('schema', case_schema)          \n",
    " .format('csv')\n",
    " .load('./sa311/case.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept = (spark.read\n",
    " .option('header', True)\n",
    " .option('inferSchema', True)\n",
    " .format('csv')\n",
    " .load('./sa311/dept.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source = (spark.read\n",
    " .option('header', True)\n",
    " .option('inferSchema', True)\n",
    " .format('csv')\n",
    " .load('./sa311/source.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date|SLA_due_date|case_late|      num_days_late|case_closed|   dept_division|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|1014127332|     1/1/18 0:42|    1/1/18 12:29|9/26/20 0:42|       NO| -998.5087616000001|        YES|Field Operations|        Stray Animal|      999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|               5|\n",
      "|1014127333|     1/1/18 0:46|     1/3/18 8:11| 1/5/18 8:30|       NO|-2.0126041669999997|        YES|     Storm Water|Removal Of Obstru...|4.322222222|     Closed| svcCRMSS|2215  GOLIAD RD, ...|               3|\n",
      "|1014127334|     1/1/18 0:48|     1/2/18 7:57| 1/5/18 8:30|       NO|       -3.022337963|        YES|     Storm Water|Removal Of Obstru...|4.320729167|     Closed| svcCRMSS|102  PALFREY ST W...|               3|\n",
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_case.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write df_case and df_dept back to disk into their own directories (my_cases and my_depts)\n",
    "\n",
    "Write df_case and df_dept to parquet files (my_cases_parquet and my_depts_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case.write.format('com.databricks.spark.csv') \\\n",
    "  .mode('overwrite').option(\"header\", \"true\").save('./sa311/my_cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept.write.format('com.databricks.spark.csv') \\\n",
    "  .mode('overwrite').option(\"header\", \"true\").save('./sa311/my_depts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read your parquet files back into your spark environment.\n",
    "\n",
    "Read case.csv and dept.csv into a pandas dataframe. (cases_pdf, depts_pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
